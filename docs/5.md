---
title: "Experimento com o Unsloth e parler tts"
permalink: /5/
layout: default
---

<style>
  .wrapper,
  .markdown-body, .inner, #main_content {
    max-width: 90% !important;
    padding: 1rem 2rem !important;
  }
  .markdown-body table {
    width: 100%;
    border-collapse: collapse;
    margin-bottom: 1rem;
  }
  .markdown-body th,
  .markdown-body td {
    border: 1px solid #ccc;
    padding: 0.5rem;
  }
  .markdown-body th {
    background: #f5f5f5;
    text-align: left;
  }
  .experiment-image, 
  .markdown-body img {
    display: block;
    margin: 1.5rem auto;
    max-width: 90%;
    border: 1px solid #ddd;
    border-radius: 6px;
  }
</style>

# Relatório de Fine-tuning do F5-TTS

Bem-vindo ao relatório do meu experimento de *fine-tuning* usando o **F5-TTS** com o corpus em português. Repositório: https://github.com/SWivid/F5-TTS

---


# Relatório de Testes – Unsloth

Durante esta semana, realizei experimentos com o **Unsloth** no contexto do **Google Colab**.  
O objetivo foi explorar como funciona o processo de *fine-tuning* em modelos de **Text-to-Speech (TTS)** disponibilizados pela ferramenta.  

Consegui rodar os notebooks no Colab de forma relativamente tranquila. Os exemplos fornecidos já apresentam o passo a passo de como executar o *fine-tuning*, o que facilitou bastante os testes iniciais. Achei o tempo de execução razoavelmente rápido, considerando a complexidade do processo.  

Porém, os resultados finais não foram tão satisfatórios. Acredito que isso se deve principalmente aos parâmetros escolhidos, que foram simplificados apenas para testes iniciais, além do pouco tempo que tive disponível nesta semana para ajustes e repetições do experimento. Ainda assim, foi possível validar que o fluxo funciona e que há potencial para melhorias em execuções futuras.  

Outro ponto os collabs em alguns casos acabaram por não funcionar completamente por falta de crédito. Talvez no contexto local seja melhor para mim.

---

## Exemplos de Saída

Abaixo segue uma tabela com os textos utilizados e os áudios gerados no experimento. Os áudios estão organizados sequencialmente nos arquivos `1.wav`, `2.wav`, `3.wav` etc.

| Texto | Áudio |
|-------|-------|
|"Hey there my name is Elise, \<giggles\> and I'm a speech generation model that can sound like a person."| <audio controls src="../audios/experimento_finetune_unsloth/1.wav"></audio>  |
|"Este é um segundo teste"    | <audio controls src="../audios/experimento_finetune_unsloth/2.wav"></audio>  |
|"O arquivo foi salvo em 24 de agosto de 2025, às 09h05."    | <audio controls src="../audios/experimento_finetune_unsloth/3.wav"></audio>  |
|"O arquivo foi salvo em 24 de agosto de 2025, às 09h05."    | <audio controls src="../audios/experimento_finetune_unsloth/4.wav"></audio>  |
|"Begin the recording at 2:30 PM and verify the microphone level."    | <audio controls src="../audios/experimento_finetune_unsloth/5.wav"></audio>  |
|"Update the audio driver and restart the system after installation."    | <audio controls src="../audios/experimento_finetune_unsloth/6.wav"></audio>  |




# Parler TTS
O Parler-TTS é um modelo de síntese de voz desenvolvido pela Hugging Face que se destaca por ser condicionado por descrições textuais. Em vez de apenas gerar fala a partir de texto, o sistema permite inserir instruções adicionais — como estilo, emoção, ritmo ou entonação — que orientam a forma como a voz será produzida. Essa abordagem o torna especialmente flexível, pois é possível criar falas mais naturais, expressivas e adaptadas a diferentes contextos. Além disso, já existem versões treinadas para o português brasileiro, o que abre espaço para experimentos de personalização, como a adaptação para sotaques regionais.


https://huggingface.co/freds0/parler-tts-mini-v1.1-ptbr
Treinado com 200 horas de audio português de:  https://huggingface.co/datasets/freds0/BRSpeech-TTS




| Texto | Áudio |
|-------|-------|
|"Minha terra tem palmeiras onde canta o sabiá, as aves que aqui gorjeiam não gorjeiam como lá." ; "Diana's speech is very clear and has a consistent tone, and she is in a very confined sounding environment with clear audio quality."| <audio controls src="../audios/experimento_parlertts/1.wav"></audio>  |
|"Eu gostaria mesmo é estar comendo uma buchada de bode na minha terrinha distante."; "Eduardo's speech is very clear and has a consistent tone, his accent is from brazils northeast and he is in a very confined sounding environment with clear audio quality."| <audio controls src="../audios/experimento_parlertts/2.wav"></audio>  |

É possível fazer finetuning também a partir deste checkpoint.


Para rodar:

``` install
pip install git+https://github.com/huggingface/parler-tts.git
```


``` script
import torch
from parler_tts import ParlerTTSForConditionalGeneration
from transformers import AutoTokenizer
import soundfile as sf

device = "cuda:0" if torch.cuda.is_available() else "cpu"
#Claro\! O resultado da execução do código, que cria a lista a partir dos nomes do dicionário, é:

speakers = ['Diana', 'Eduardo', 'Uriel', 'Guilherme', 'Zoe', 'Quirino', 'Enzo', 'Henrique', 'Davi', 'Thiago', 'Sofia', 'Wilson', 'Ximenia', 'Osvaldo', 'Icaro', 'Raquel', 'Carolina', 'Debora', 'Xavier', 'Olivia', 'Erick', 'Silvio', 'Daniel', 'Joao', 'Italo', 'Manuela', 'Caio', 'Yuri', 'Samuel', 'Isabela', 'Sabrina', 'Paulo', 'Victor', 'Amanda', 'Alexandre', 'Vitoria', 'Emilia', 'Diego', 'Daniela', 'Joana', 'Gabriel', 'Wanda', 'Alice', 'Valentina', 'Bruno', 'Hugo', 'Yara', 'Vinicius', 'Patricia', 'Luiz', 'Valter', 'Leticia', 'Iris', 'Fernando', 'Julio', 'Carlos', 'Lucas', 'Mateus', 'Rebeca', 'Rafael', 'Bernardo', 'Flavia']

model = ParlerTTSForConditionalGeneration.from_pretrained("freds0/parler-tts-mini-v1.1-ptbr").to(device)
tokenizer = AutoTokenizer.from_pretrained("parler-tts/parler-tts-mini-multilingual")
description_tokenizer = AutoTokenizer.from_pretrained(model.config.text_encoder._name_or_path)

prompt = "Minha terra tem palmeiras onde canta o sabiá, as aves que aqui gorjeiam não gorjeiam como lá."
description = "Diana's speech is very clear and has a consistent tone, and she is in a very confined sounding environment with clear audio quality."

input_ids = description_tokenizer(description, return_tensors="pt").input_ids.to(device)
prompt_input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)
audio_arr = generation.cpu().numpy().squeeze()
sf.write("parler_tts_out.wav", audio_arr, model.config.sampling_rate)


```

